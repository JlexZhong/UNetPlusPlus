{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzIHe66asnEb"
      },
      "source": [
        "# 训练U-Net++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfEb_8AGstsT"
      },
      "source": [
        "## 配置pytorch环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkXGHMh7qmny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ac4889-f6c7-47fe-dd6b-36098b5e4bfd"
      },
      "source": [
        "%cd /content/drive/MyDrive/U-Net++"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/U-Net++\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkXT0enLHMO1"
      },
      "source": [
        "!pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFGV8aNLi747"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkmy36ghsw_y"
      },
      "source": [
        "## 导入包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOFpTHRDsYBI"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from glob import glob\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# pip install PyYaml\n",
        "import yaml\n",
        "# https://github.com/albumentations-team/albumentations\n",
        "# pip install -U albumentations\n",
        "# python3.6+\n",
        "from albumentations.augmentations.geometric.rotate import RandomRotate90\n",
        "from albumentations.augmentations.geometric.resize import Resize \n",
        "from albumentations.augmentations import transforms\n",
        "from albumentations.core.composition import Compose, OneOf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import archs\n",
        "import losses\n",
        "from dataset import Dataset\n",
        "from metrics import iou_score\n",
        "from utils import AverageMeter, str2bool\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvr81UFZs7Hz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b5906af-4b7b-47c9-ae02-13fe9a152ec9"
      },
      "source": [
        "\n",
        "ARCH_NAMES = archs.__all__\n",
        "LOSS_NAMES = losses.__all__\n",
        "LOSS_NAMES.append('BCEWithLogitsLoss')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "指定参数：\n",
        "--dataset dsb2018_96 \n",
        "--arch NestedUNet\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n指定参数：\\n--dataset dsb2018_96 \\n--arch NestedUNet\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ty_OOuM2z9c"
      },
      "source": [
        "def train(config, train_loader, model, criterion, optimizer):\n",
        "    avg_meters = {'loss': AverageMeter(),\n",
        "                  'iou': AverageMeter()}\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader))\n",
        "    for input, target, _ in train_loader:\n",
        "        #input = input.cuda()\n",
        "        #target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        if config.DEEP_SUPERVISION:\n",
        "            outputs = model(input)\n",
        "            loss = 0\n",
        "            for output in outputs:\n",
        "                loss += criterion(output, target)\n",
        "            loss /= len(outputs)\n",
        "            iou = iou_score(outputs[-1], target)\n",
        "        else:\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            iou = iou_score(output, target)\n",
        "\n",
        "        # compute gradient and do optimizing step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
        "        avg_meters['iou'].update(iou, input.size(0))\n",
        "\n",
        "        postfix = OrderedDict([\n",
        "            ('loss', avg_meters['loss'].avg),\n",
        "            ('iou', avg_meters['iou'].avg),\n",
        "        ])\n",
        "        pbar.set_postfix(postfix)\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "\n",
        "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
        "                        ('iou', avg_meters['iou'].avg)])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBJeuDmP22IE"
      },
      "source": [
        "def validate(config, val_loader, model, criterion):\n",
        "    avg_meters = {'loss': AverageMeter(),\n",
        "                  'iou': AverageMeter()}\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(total=len(val_loader))\n",
        "        for input, target, _ in val_loader:\n",
        "            # input = input.cuda()\n",
        "            # target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            if config.DEEP_SUPERVISION:\n",
        "                outputs = model(input)\n",
        "                loss = 0\n",
        "                for output in outputs:\n",
        "                    loss += criterion(output, target)\n",
        "                loss /= len(outputs)\n",
        "                iou = iou_score(outputs[-1], target)\n",
        "            else:\n",
        "                output = model(input)\n",
        "                loss = criterion(output, target)\n",
        "                iou = iou_score(output, target)\n",
        "\n",
        "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
        "            avg_meters['iou'].update(iou, input.size(0))\n",
        "\n",
        "            postfix = OrderedDict([\n",
        "                ('loss', avg_meters['loss'].avg),\n",
        "                ('iou', avg_meters['iou'].avg),\n",
        "            ])\n",
        "            pbar.set_postfix(postfix)\n",
        "            pbar.update(1)\n",
        "        pbar.close()\n",
        "\n",
        "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
        "                        ('iou', avg_meters['iou'].avg)])\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELZ5Wp5SxQnF"
      },
      "source": [
        "class Config(object):\n",
        "  NAME = \"CT_Rock\"\n",
        "  EPOCHS = 50\n",
        "  BATCH_SIZE = 8\n",
        "  ARCH = 'NestedUNet'\n",
        "  DEEP_SUPERVISION = False\n",
        "  INPUT_CHANNELS = 3\n",
        "  NUM_CLASSES = 1 \n",
        "  INPUT_W = 512\n",
        "  INPUT_H = 512\n",
        "  LOSS = \"BCEDiceLoss\"\n",
        "  DATASET = \"final_rock_dataset\"\n",
        "  IMG_EXT = '.png'\n",
        "  MASK_EXT = '.png'\n",
        "  # choices=['Adam', 'SGD'],\n",
        "  OPTIMIZER = 'Adam'\n",
        "  # initial learning rate\n",
        "  LR = 1e-3\n",
        "  MOMENTUM = 0.9\n",
        "  WEIGHT_DECAY = 1e-4\n",
        "  NESTEROV = False\n",
        "  # choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])\n",
        "  SCHEDULER = 'CosineAnnealingLR'\n",
        "  # minimum learning rate\n",
        "  MIN_LR = 1e-5\n",
        "  FACTOR = 0.1\n",
        "  PATIENCE = 2\n",
        "  MILESTONES = '1,2'\n",
        "  GAMMA = 2/3\n",
        "  # early stopping (default: -1)\n",
        "  EARLY_STOPPING = -1\n",
        "  NUM_WORKERS = 0\n",
        "\n",
        "  def display(self):\n",
        "    \"\"\"Display Configuration values.\"\"\"\n",
        "    print(\"\\nConfigurations:\")\n",
        "    for a in dir(self):\n",
        "      if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
        "        print(\"{:30} {}\".format(a, getattr(self, a)))\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN7sqkoK0j2S",
        "outputId": "5788bbac-7e67-45b7-dede-f68c9b42617f"
      },
      "source": [
        "config = Config()\n",
        "config.display()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configurations:\n",
            "ARCH                           NestedUNet\n",
            "BATCH_SIZE                     8\n",
            "DATASET                        final_rock_dataset\n",
            "DEEP_SUPERVISION               False\n",
            "EARLY_STOPPING                 -1\n",
            "EPOCHS                         50\n",
            "FACTOR                         0.1\n",
            "GAMMA                          0.6666666666666666\n",
            "IMG_EXT                        .png\n",
            "INPUT_CHANNELS                 3\n",
            "INPUT_H                        512\n",
            "INPUT_W                        512\n",
            "LOSS                           BCEDiceLoss\n",
            "LR                             0.001\n",
            "MASK_EXT                       .png\n",
            "MILESTONES                     1,2\n",
            "MIN_LR                         1e-05\n",
            "MOMENTUM                       0.9\n",
            "NAME                           CT_Rock\n",
            "NESTEROV                       False\n",
            "NUM_CLASSES                    1\n",
            "NUM_WORKERS                    0\n",
            "OPTIMIZER                      Adam\n",
            "PATIENCE                       2\n",
            "SCHEDULER                      CosineAnnealingLR\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4serD86s6-D"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj7UunjJ05Oa"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji0hdjCas65c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2617ea33-2888-461b-ded0-82af2d73dd2b"
      },
      "source": [
        "\n",
        "if config.NAME is None:\n",
        "    if config.DEEP_SUPERVISION:\n",
        "        config.NAME = '%s_%s_wDS' % (config.DATASET, config.ARCH)\n",
        "    else:\n",
        "        config.NAME = '%s_%s_woDS' % (config.DATASET, config.ARCH)\n",
        "os.makedirs('models/%s' % config.NAME, exist_ok=True)\n",
        "\n",
        "\n",
        "# with open('models/%s/config.yml' % config.NAME, 'w') as f:\n",
        "#     yaml.dump(config, f)\n",
        "\n",
        "# define loss function (criterion)\n",
        "if config.LOSS == 'BCEWithLogitsLoss':\n",
        "    criterion = nn.BCEWithLogitsLoss().cuda()#WithLogits 就是先将输出结果经过sigmoid再交叉熵\n",
        "else:\n",
        "    criterion = losses.__dict__[config.LOSS]().cuda()\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# create model\n",
        "print(\"=> creating model %s\" % config.ARCH)\n",
        "model = archs.__dict__[config.ARCH](config.NUM_CLASSES,\n",
        "                    config.INPUT_CHANNELS,\n",
        "                    config.DEEP_SUPERVISION)\n",
        "# ======================================================================================#\n",
        "# model = model.cuda()  # 使用GPU\n",
        "\n",
        "params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "if config.OPTIMIZER == 'Adam':\n",
        "    optimizer = optim.Adam(\n",
        "        params, lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
        "elif config.OPTIMIZER == 'SGD':\n",
        "    optimizer = optim.SGD(params, lr=config.LR, momentum=config.MOMENTUM,\n",
        "                          nesterov=config.NESTEROV, weight_decay=config.WEIGHT_DECAY)\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "if config.SCHEDULER == 'CosineAnnealingLR':\n",
        "    scheduler = lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=config.EPOCHS, eta_min=config.MIN_LR)\n",
        "elif config.SCHEDULER == 'ReduceLROnPlateau':\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config.FACTOR, patience=config.PATIENCE,\n",
        "                                                verbose=1, min_lr=config.MIN_LR)\n",
        "elif config.SCHEDULER == 'MultiStepLR':\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config.MILESTONES.split(',')], gamma=config.GAMMA)\n",
        "elif config.SCHEDULER == 'ConstantLR':\n",
        "    scheduler = None\n",
        "else:\n",
        "    raise NotImplementedError\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> creating model NestedUNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGqN3TwMJO0_"
      },
      "source": [
        "获取数据集信息"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waw1qt-1F5id",
        "outputId": "0e705066-2ce5-4f7e-f29e-9ba0a98377d6"
      },
      "source": [
        "# Data loading code\n",
        "img_path = os.path.join('inputs', config.DATASET, 'images', '*' + config.IMG_EXT)\n",
        "print(img_path)\n",
        "img_ids = glob(img_path)\n",
        "img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n",
        "print(img_ids)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs/final_rock_dataset/images/*.png\n",
            "['3', '1', '2', '9', '7', '10', '4', '13', '11', '12', '8', '6', '5', '14', '15', '22', '23', '17', '20', '21', '18', '16', '19', '24', '31', '28', '27', '30', '29', '26', '25', '33', '35', '32', '34', '36', '45', '44', '37', '39', '42', '41', '43', '38', '40', '47', '46', '49', '48', '50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5t6GAYwJJn1"
      },
      "source": [
        "划分训练集、验证集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0UueKUfI1x0",
        "outputId": "6721e90d-d843-484a-8dd1-793ea6bd3705"
      },
      "source": [
        "train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)\n",
        "print('train_dataset=========>\\n',train_img_ids,'\\n')\n",
        "print('val_dataset=========>\\n',val_img_ids,'\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset=========>\n",
            " ['47', '26', '10', '33', '39', '25', '32', '49', '40', '7', '38', '14', '21', '42', '30', '2', '41', '17', '37', '43', '45', '46', '6', '31', '29', '19', '18', '4', '48', '9', '27', '16', '24', '28', '1', '23', '34', '5', '36', '3'] \n",
            "\n",
            "val_dataset=========>\n",
            " ['50', '20', '12', '15', '35', '8', '11', '22', '44', '13'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXDS9QQbJcX0"
      },
      "source": [
        "### 数据增强"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRk3HkVAHkOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736549e6-99eb-4b4f-9a80-8d83624a69bc"
      },
      "source": [
        "#数据增强：\n",
        "train_transform = Compose([\n",
        "    RandomRotate90(),\n",
        "    transforms.Flip(),\n",
        "    OneOf([\n",
        "        transforms.HueSaturationValue(),\n",
        "        transforms.RandomBrightness(),\n",
        "        transforms.RandomContrast(),\n",
        "    ], p=1),#按照归一化的概率选择执行哪一个\n",
        "    Resize(config.INPUT_H, config.INPUT_W),\n",
        "    transforms.Normalize(),\n",
        "])\n",
        "\n",
        "val_transform = Compose([\n",
        "    Resize(config.INPUT_H, config.INPUT_W),\n",
        "    transforms.Normalize(),\n",
        "])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1746: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1772: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq5I85CeJWBQ"
      },
      "source": [
        "\n",
        "train_dataset = Dataset(\n",
        "    img_ids=train_img_ids,\n",
        "    img_dir=os.path.join('inputs', config.DATASET, 'images'),\n",
        "    mask_dir=os.path.join('inputs', config.DATASET, 'masks'),\n",
        "    img_ext=config.IMG_EXT,\n",
        "    mask_ext=config.MASK_EXT,\n",
        "    num_classes=config.NUM_CLASSES,\n",
        "    transform=train_transform)\n",
        "val_dataset = Dataset(\n",
        "    img_ids=val_img_ids,\n",
        "    img_dir=os.path.join('inputs', config.DATASET, 'images'),\n",
        "    mask_dir=os.path.join('inputs', config.DATASET, 'masks'),\n",
        "    img_ext=config.IMG_EXT,\n",
        "    mask_ext=config.IMG_EXT,\n",
        "    num_classes=config.NUM_CLASSES,\n",
        "    transform=val_transform)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr_z3s8wKEPF"
      },
      "source": [
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=True,  # 打乱\n",
        "    num_workers=config.NUM_CLASSES,  # 数据集较小时，使用默认\n",
        "    drop_last=True)#不能整除的batch是否就不要了\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=config.NUM_CLASSES,\n",
        "    drop_last=True)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFpMchbJQiT6",
        "outputId": "cb737a66-fcbc-42b5-979d-bd8839c4db03"
      },
      "source": [
        "\n",
        "log = OrderedDict([\n",
        "    ('epoch', []),\n",
        "    ('lr', []),\n",
        "    ('loss', []),\n",
        "    ('iou', []),\n",
        "    ('val_loss', []),\n",
        "    ('val_iou', []),\n",
        "])\n",
        "\n",
        "best_iou = 0\n",
        "trigger = 0\n",
        "for epoch in range(config.EPOCHS):\n",
        "    print('Epoch [%d/%d]' % (epoch, config.EPOCHS))\n",
        "\n",
        "    # train for one epoch\n",
        "    train_log = train(config, train_loader, model, criterion, optimizer)\n",
        "    # evaluate on validation set\n",
        "    val_log = validate(config, val_loader, model, criterion)\n",
        "\n",
        "    if config.SCHEDULER == 'CosineAnnealingLR':\n",
        "        scheduler.step()\n",
        "    elif config.SCHEDULER == 'ReduceLROnPlateau':\n",
        "        scheduler.step(val_log['loss'])\n",
        "\n",
        "    print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
        "          % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
        "\n",
        "    log['epoch'].append(epoch)\n",
        "    log['lr'].append(config.LR)\n",
        "    log['loss'].append(train_log['loss'])\n",
        "    log['iou'].append(train_log['iou'])\n",
        "    log['val_loss'].append(val_log['loss'])\n",
        "    log['val_iou'].append(val_log['iou'])\n",
        "\n",
        "    pd.DataFrame(log).to_csv('models/%s/log.csv' %\n",
        "                              config.NAME, index=False)\n",
        "\n",
        "    trigger += 1\n",
        "\n",
        "    if val_log['iou'] > best_iou:\n",
        "        torch.save(model.state_dict(), 'models/%s/model.pth' %\n",
        "                    config.NAME)\n",
        "        best_iou = val_log['iou']\n",
        "        print(\"=> saved best model\")\n",
        "        trigger = 0\n",
        "\n",
        "    # early stopping\n",
        "    if config.EARLY_STOPPING >= 0 and trigger >= config.EARLY_STOPPING:\n",
        "        print(\"=> early stopping\")\n",
        "        break\n",
        "\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7059LYYwYaD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}